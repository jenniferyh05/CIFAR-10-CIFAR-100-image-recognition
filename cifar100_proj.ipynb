{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar100_proj.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1JHEzggQSKKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "197009dd-5c1c-4730-de67-8258ec44b073"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar100\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras.callbacks as cb\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.utils import np_utils\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zTN3I1p3Sa5d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def PreprocessDataset():\n",
        "    (x_train_org, y_train_org), (x_test_org, y_test_org) = cifar100.load_data(label_mode='fine')\n",
        "    x_train_org = x_train_org.astype('float32')\n",
        "    x_test_org = x_test_org.astype('float32')\n",
        "    # Normalize value to [0, 1]\n",
        "    #x_train_org /= 255\n",
        "    #x_test_org /= 255\n",
        "    # Transform lables to one-hot # for dl, no need to do normalization here because \n",
        "    y_train_org = np_utils.to_categorical(y_train_org, 100)\n",
        "    y_test_org = np_utils.to_categorical(y_test_org, 100)\n",
        "    # Reshape: here x_train is re-shaped to [width] Ã— [height] x [channel]\n",
        "    x_train_org = x_train_org.reshape(x_train_org.shape[0], 32, 32, 3) # maybe not needed \n",
        "    x_test_org = x_test_org.reshape(x_test_org.shape[0], 32, 32, 3) #maybe not needed because original shape already\n",
        "    return [x_train_org, x_test_org, y_train_org, y_test_org]\n",
        "\n",
        "x_train_org, x_test_org, y_train_org, y_test_org = PreprocessDataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqBzVmZhA8-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa47a466-971e-4e5a-ebac-d493d647df10"
      },
      "cell_type": "code",
      "source": [
        "y_train_org.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "YYTpCSarSe6c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CIFAR100_LABELS_LIST = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
        "    'worm'\n",
        "]\n",
        "\n",
        "CIFAR100_LABELS_LIST.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rLvo3QVfSgUk",
        "colab_type": "code",
        "outputId": "51ffe187-aac0-411b-874b-3fb8574871c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "label_choosen = ['bridge', 'castle', 'house', 'road', 'skyscraper','cloud', 'forest', 'mountain', 'plain','sea']\n",
        "\n",
        "label_loc = []\n",
        "for item in CIFAR100_LABELS_LIST:\n",
        "  if item in label_choosen:\n",
        "    print([CIFAR100_LABELS_LIST.index(item), item])\n",
        "    label_loc.append(CIFAR100_LABELS_LIST.index(item))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[12, 'bridge']\n",
            "[17, 'castle']\n",
            "[23, 'cloud']\n",
            "[33, 'forest']\n",
            "[37, 'house']\n",
            "[49, 'mountain']\n",
            "[60, 'plain']\n",
            "[68, 'road']\n",
            "[71, 'sea']\n",
            "[76, 'skyscraper']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UVPRQ4lkSp73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sub_class selection -- return dictionary (for later milestone 2 usage)\n",
        "def subclass(x, y):\n",
        "  x_dict = {}\n",
        "  y_dict = {}\n",
        "  for i, item in enumerate(y):\n",
        "    if np.argmax(item) in label_loc:\n",
        "      if np.argmax(item) in x_dict.keys():\n",
        "        x_dict[np.argmax(item)].append(x[i])\n",
        "        y_dict[np.argmax(item)].append(y[i])\n",
        "      else:\n",
        "        x_dict[np.argmax(item)] = []\n",
        "        x_dict[np.argmax(item)].append(x[i])\n",
        "        y_dict[np.argmax(item)] = []\n",
        "        y_dict[np.argmax(item)].append(y[i])\n",
        "  return x_dict, y_dict\n",
        "\n",
        "x_train_dict, y_train_dict = subclass(x_train_org, y_train_org)\n",
        "x_test_dict, y_test_dict = subclass(x_test_org, y_test_org)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gd1LHrHJEeeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40344194-c33d-4125-9cee-771234e732b2"
      },
      "cell_type": "code",
      "source": [
        "y_train_dict[12][0].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "iXqV3e9XSsgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "108d9dff-7200-41fd-bd98-ab19a1b66000"
      },
      "cell_type": "code",
      "source": [
        "# return np.array\n",
        "def subclass_np(x_dict, y_dict):\n",
        "  x = []\n",
        "  y= []\n",
        "  for key in x_dict.keys():\n",
        "    x.append(x_dict[key])\n",
        "    y.append(y_dict[key])\n",
        "  x = np.array(x,dtype=np.float32)  \n",
        "  x = x.reshape(x.shape[0]*x.shape[1], 32,32,3)\n",
        "  y = np.array(y)\n",
        "  y =  y.reshape(y.shape[0]*y.shape[1], 100,1)\n",
        "  print(y.shape)\n",
        "  return x,y\n",
        "x_train, y_train_list = subclass_np(x_train_dict, y_train_dict)\n",
        "x_test,  y_test_list = subclass_np(x_test_dict, y_test_dict)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 100, 1)\n",
            "(1000, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yuatGJBUSy-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make y_train_list and y_test_list ready\n",
        "binary_label_loc_0 = [12,17,37,68,76]\n",
        "binary_label_0 = ['bridge', 'castle', 'house', 'road', 'skyscraper']\n",
        "def y_binary(y_list):\n",
        "  y_bi = []\n",
        "  for item in y_list:\n",
        "    if np.argmax(item) in binary_label_loc_0:\n",
        "      y_bi.append(0)\n",
        "    else:\n",
        "      y_bi.append(1)\n",
        "  return np.array(y_bi)\n",
        "\n",
        "y_train = y_binary(y_train_list)\n",
        "y_test = y_binary(y_test_list)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TxuP7aYWS0lu",
        "colab_type": "code",
        "outputId": "6c27d852-4dcf-4045-eba4-f5f1411994a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print([x_train.shape, y_train.shape, x_test.shape, y_test.shape])\n",
        "print([np.unique(y_train),np.bincount(y_train)])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(5000, 32, 32, 3), (5000,), (1000, 32, 32, 3), (1000,)]\n",
            "[array([0, 1]), array([2500, 2500])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hznBMqTqTEbb",
        "colab_type": "code",
        "outputId": "251e2d38-7f21-48e8-bbb5-d2e8c3e5071c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "n = 150\n",
        "\n",
        "for i in range(5):\n",
        "  n = n+1\n",
        "  plt.subplot(n)\n",
        "  plt.imshow(x_train_dict[12][i])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noUtSIf4PKo/MmVB43A8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "v9XDAfKNUfib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "fb747b5a-3862-4aae-8f42-6e40af613dfb"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.densenet import DenseNet201,preprocess_input\n",
        "# create the base pre-trained model\n",
        "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(32,32,3))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s2573c-_VrBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "densenet_input_train = preprocess_input(x_train)\n",
        "densenet_input_test = preprocess_input(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wdgXUbBWrMD",
        "colab_type": "code",
        "outputId": "bd2ca46a-7c45-4af7-9204-736495dca4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# and a logistic layer -- here we have 2 classes\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional DenseNet201 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(len(model.layers))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mo2EEUkFX7pH",
        "colab_type": "code",
        "outputId": "d1a3d72f-c9a3-4e6f-f6b9-113211aa1434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "cell_type": "code",
      "source": [
        "# train the model on the new data for a few epochs\n",
        "model.fit(densenet_input_train, to_categorical(y_train, num_classes=2), batch_size=64, epochs=2,\n",
        "          validation_data=(densenet_input_test, to_categorical(y_test, num_classes=2)),\n",
        "          verbose=2, shuffle=True)\n",
        "# at this point, the top layers are well trained and we can start fine-tuning\n",
        "# convolutional layers from DenseNet201. We will freeze the bottom N layers\n",
        "# and train the remaining top layers."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 5000 samples, validate on 1000 samples\n",
            "Epoch 1/2\n",
            " - 15s - loss: 0.9086 - acc: 0.6900 - val_loss: 0.6184 - val_acc: 0.7440\n",
            "Epoch 2/2\n",
            " - 3s - loss: 0.5030 - acc: 0.7688 - val_loss: 0.4953 - val_acc: 0.8110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5562e02400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "BYSNQC83bGJK",
        "colab_type": "code",
        "outputId": "39d44163-d3d0-4783-84bc-b64b1065dc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11894
        }
      },
      "cell_type": "code",
      "source": [
        "# let's visualize layer names and layer indices to see how many layers\n",
        "# we should freeze:\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_1\n",
            "1 zero_padding2d_1\n",
            "2 conv1/conv\n",
            "3 conv1/bn\n",
            "4 conv1/relu\n",
            "5 zero_padding2d_2\n",
            "6 pool1\n",
            "7 conv2_block1_0_bn\n",
            "8 conv2_block1_0_relu\n",
            "9 conv2_block1_1_conv\n",
            "10 conv2_block1_1_bn\n",
            "11 conv2_block1_1_relu\n",
            "12 conv2_block1_2_conv\n",
            "13 conv2_block1_concat\n",
            "14 conv2_block2_0_bn\n",
            "15 conv2_block2_0_relu\n",
            "16 conv2_block2_1_conv\n",
            "17 conv2_block2_1_bn\n",
            "18 conv2_block2_1_relu\n",
            "19 conv2_block2_2_conv\n",
            "20 conv2_block2_concat\n",
            "21 conv2_block3_0_bn\n",
            "22 conv2_block3_0_relu\n",
            "23 conv2_block3_1_conv\n",
            "24 conv2_block3_1_bn\n",
            "25 conv2_block3_1_relu\n",
            "26 conv2_block3_2_conv\n",
            "27 conv2_block3_concat\n",
            "28 conv2_block4_0_bn\n",
            "29 conv2_block4_0_relu\n",
            "30 conv2_block4_1_conv\n",
            "31 conv2_block4_1_bn\n",
            "32 conv2_block4_1_relu\n",
            "33 conv2_block4_2_conv\n",
            "34 conv2_block4_concat\n",
            "35 conv2_block5_0_bn\n",
            "36 conv2_block5_0_relu\n",
            "37 conv2_block5_1_conv\n",
            "38 conv2_block5_1_bn\n",
            "39 conv2_block5_1_relu\n",
            "40 conv2_block5_2_conv\n",
            "41 conv2_block5_concat\n",
            "42 conv2_block6_0_bn\n",
            "43 conv2_block6_0_relu\n",
            "44 conv2_block6_1_conv\n",
            "45 conv2_block6_1_bn\n",
            "46 conv2_block6_1_relu\n",
            "47 conv2_block6_2_conv\n",
            "48 conv2_block6_concat\n",
            "49 pool2_bn\n",
            "50 pool2_relu\n",
            "51 pool2_conv\n",
            "52 pool2_pool\n",
            "53 conv3_block1_0_bn\n",
            "54 conv3_block1_0_relu\n",
            "55 conv3_block1_1_conv\n",
            "56 conv3_block1_1_bn\n",
            "57 conv3_block1_1_relu\n",
            "58 conv3_block1_2_conv\n",
            "59 conv3_block1_concat\n",
            "60 conv3_block2_0_bn\n",
            "61 conv3_block2_0_relu\n",
            "62 conv3_block2_1_conv\n",
            "63 conv3_block2_1_bn\n",
            "64 conv3_block2_1_relu\n",
            "65 conv3_block2_2_conv\n",
            "66 conv3_block2_concat\n",
            "67 conv3_block3_0_bn\n",
            "68 conv3_block3_0_relu\n",
            "69 conv3_block3_1_conv\n",
            "70 conv3_block3_1_bn\n",
            "71 conv3_block3_1_relu\n",
            "72 conv3_block3_2_conv\n",
            "73 conv3_block3_concat\n",
            "74 conv3_block4_0_bn\n",
            "75 conv3_block4_0_relu\n",
            "76 conv3_block4_1_conv\n",
            "77 conv3_block4_1_bn\n",
            "78 conv3_block4_1_relu\n",
            "79 conv3_block4_2_conv\n",
            "80 conv3_block4_concat\n",
            "81 conv3_block5_0_bn\n",
            "82 conv3_block5_0_relu\n",
            "83 conv3_block5_1_conv\n",
            "84 conv3_block5_1_bn\n",
            "85 conv3_block5_1_relu\n",
            "86 conv3_block5_2_conv\n",
            "87 conv3_block5_concat\n",
            "88 conv3_block6_0_bn\n",
            "89 conv3_block6_0_relu\n",
            "90 conv3_block6_1_conv\n",
            "91 conv3_block6_1_bn\n",
            "92 conv3_block6_1_relu\n",
            "93 conv3_block6_2_conv\n",
            "94 conv3_block6_concat\n",
            "95 conv3_block7_0_bn\n",
            "96 conv3_block7_0_relu\n",
            "97 conv3_block7_1_conv\n",
            "98 conv3_block7_1_bn\n",
            "99 conv3_block7_1_relu\n",
            "100 conv3_block7_2_conv\n",
            "101 conv3_block7_concat\n",
            "102 conv3_block8_0_bn\n",
            "103 conv3_block8_0_relu\n",
            "104 conv3_block8_1_conv\n",
            "105 conv3_block8_1_bn\n",
            "106 conv3_block8_1_relu\n",
            "107 conv3_block8_2_conv\n",
            "108 conv3_block8_concat\n",
            "109 conv3_block9_0_bn\n",
            "110 conv3_block9_0_relu\n",
            "111 conv3_block9_1_conv\n",
            "112 conv3_block9_1_bn\n",
            "113 conv3_block9_1_relu\n",
            "114 conv3_block9_2_conv\n",
            "115 conv3_block9_concat\n",
            "116 conv3_block10_0_bn\n",
            "117 conv3_block10_0_relu\n",
            "118 conv3_block10_1_conv\n",
            "119 conv3_block10_1_bn\n",
            "120 conv3_block10_1_relu\n",
            "121 conv3_block10_2_conv\n",
            "122 conv3_block10_concat\n",
            "123 conv3_block11_0_bn\n",
            "124 conv3_block11_0_relu\n",
            "125 conv3_block11_1_conv\n",
            "126 conv3_block11_1_bn\n",
            "127 conv3_block11_1_relu\n",
            "128 conv3_block11_2_conv\n",
            "129 conv3_block11_concat\n",
            "130 conv3_block12_0_bn\n",
            "131 conv3_block12_0_relu\n",
            "132 conv3_block12_1_conv\n",
            "133 conv3_block12_1_bn\n",
            "134 conv3_block12_1_relu\n",
            "135 conv3_block12_2_conv\n",
            "136 conv3_block12_concat\n",
            "137 pool3_bn\n",
            "138 pool3_relu\n",
            "139 pool3_conv\n",
            "140 pool3_pool\n",
            "141 conv4_block1_0_bn\n",
            "142 conv4_block1_0_relu\n",
            "143 conv4_block1_1_conv\n",
            "144 conv4_block1_1_bn\n",
            "145 conv4_block1_1_relu\n",
            "146 conv4_block1_2_conv\n",
            "147 conv4_block1_concat\n",
            "148 conv4_block2_0_bn\n",
            "149 conv4_block2_0_relu\n",
            "150 conv4_block2_1_conv\n",
            "151 conv4_block2_1_bn\n",
            "152 conv4_block2_1_relu\n",
            "153 conv4_block2_2_conv\n",
            "154 conv4_block2_concat\n",
            "155 conv4_block3_0_bn\n",
            "156 conv4_block3_0_relu\n",
            "157 conv4_block3_1_conv\n",
            "158 conv4_block3_1_bn\n",
            "159 conv4_block3_1_relu\n",
            "160 conv4_block3_2_conv\n",
            "161 conv4_block3_concat\n",
            "162 conv4_block4_0_bn\n",
            "163 conv4_block4_0_relu\n",
            "164 conv4_block4_1_conv\n",
            "165 conv4_block4_1_bn\n",
            "166 conv4_block4_1_relu\n",
            "167 conv4_block4_2_conv\n",
            "168 conv4_block4_concat\n",
            "169 conv4_block5_0_bn\n",
            "170 conv4_block5_0_relu\n",
            "171 conv4_block5_1_conv\n",
            "172 conv4_block5_1_bn\n",
            "173 conv4_block5_1_relu\n",
            "174 conv4_block5_2_conv\n",
            "175 conv4_block5_concat\n",
            "176 conv4_block6_0_bn\n",
            "177 conv4_block6_0_relu\n",
            "178 conv4_block6_1_conv\n",
            "179 conv4_block6_1_bn\n",
            "180 conv4_block6_1_relu\n",
            "181 conv4_block6_2_conv\n",
            "182 conv4_block6_concat\n",
            "183 conv4_block7_0_bn\n",
            "184 conv4_block7_0_relu\n",
            "185 conv4_block7_1_conv\n",
            "186 conv4_block7_1_bn\n",
            "187 conv4_block7_1_relu\n",
            "188 conv4_block7_2_conv\n",
            "189 conv4_block7_concat\n",
            "190 conv4_block8_0_bn\n",
            "191 conv4_block8_0_relu\n",
            "192 conv4_block8_1_conv\n",
            "193 conv4_block8_1_bn\n",
            "194 conv4_block8_1_relu\n",
            "195 conv4_block8_2_conv\n",
            "196 conv4_block8_concat\n",
            "197 conv4_block9_0_bn\n",
            "198 conv4_block9_0_relu\n",
            "199 conv4_block9_1_conv\n",
            "200 conv4_block9_1_bn\n",
            "201 conv4_block9_1_relu\n",
            "202 conv4_block9_2_conv\n",
            "203 conv4_block9_concat\n",
            "204 conv4_block10_0_bn\n",
            "205 conv4_block10_0_relu\n",
            "206 conv4_block10_1_conv\n",
            "207 conv4_block10_1_bn\n",
            "208 conv4_block10_1_relu\n",
            "209 conv4_block10_2_conv\n",
            "210 conv4_block10_concat\n",
            "211 conv4_block11_0_bn\n",
            "212 conv4_block11_0_relu\n",
            "213 conv4_block11_1_conv\n",
            "214 conv4_block11_1_bn\n",
            "215 conv4_block11_1_relu\n",
            "216 conv4_block11_2_conv\n",
            "217 conv4_block11_concat\n",
            "218 conv4_block12_0_bn\n",
            "219 conv4_block12_0_relu\n",
            "220 conv4_block12_1_conv\n",
            "221 conv4_block12_1_bn\n",
            "222 conv4_block12_1_relu\n",
            "223 conv4_block12_2_conv\n",
            "224 conv4_block12_concat\n",
            "225 conv4_block13_0_bn\n",
            "226 conv4_block13_0_relu\n",
            "227 conv4_block13_1_conv\n",
            "228 conv4_block13_1_bn\n",
            "229 conv4_block13_1_relu\n",
            "230 conv4_block13_2_conv\n",
            "231 conv4_block13_concat\n",
            "232 conv4_block14_0_bn\n",
            "233 conv4_block14_0_relu\n",
            "234 conv4_block14_1_conv\n",
            "235 conv4_block14_1_bn\n",
            "236 conv4_block14_1_relu\n",
            "237 conv4_block14_2_conv\n",
            "238 conv4_block14_concat\n",
            "239 conv4_block15_0_bn\n",
            "240 conv4_block15_0_relu\n",
            "241 conv4_block15_1_conv\n",
            "242 conv4_block15_1_bn\n",
            "243 conv4_block15_1_relu\n",
            "244 conv4_block15_2_conv\n",
            "245 conv4_block15_concat\n",
            "246 conv4_block16_0_bn\n",
            "247 conv4_block16_0_relu\n",
            "248 conv4_block16_1_conv\n",
            "249 conv4_block16_1_bn\n",
            "250 conv4_block16_1_relu\n",
            "251 conv4_block16_2_conv\n",
            "252 conv4_block16_concat\n",
            "253 conv4_block17_0_bn\n",
            "254 conv4_block17_0_relu\n",
            "255 conv4_block17_1_conv\n",
            "256 conv4_block17_1_bn\n",
            "257 conv4_block17_1_relu\n",
            "258 conv4_block17_2_conv\n",
            "259 conv4_block17_concat\n",
            "260 conv4_block18_0_bn\n",
            "261 conv4_block18_0_relu\n",
            "262 conv4_block18_1_conv\n",
            "263 conv4_block18_1_bn\n",
            "264 conv4_block18_1_relu\n",
            "265 conv4_block18_2_conv\n",
            "266 conv4_block18_concat\n",
            "267 conv4_block19_0_bn\n",
            "268 conv4_block19_0_relu\n",
            "269 conv4_block19_1_conv\n",
            "270 conv4_block19_1_bn\n",
            "271 conv4_block19_1_relu\n",
            "272 conv4_block19_2_conv\n",
            "273 conv4_block19_concat\n",
            "274 conv4_block20_0_bn\n",
            "275 conv4_block20_0_relu\n",
            "276 conv4_block20_1_conv\n",
            "277 conv4_block20_1_bn\n",
            "278 conv4_block20_1_relu\n",
            "279 conv4_block20_2_conv\n",
            "280 conv4_block20_concat\n",
            "281 conv4_block21_0_bn\n",
            "282 conv4_block21_0_relu\n",
            "283 conv4_block21_1_conv\n",
            "284 conv4_block21_1_bn\n",
            "285 conv4_block21_1_relu\n",
            "286 conv4_block21_2_conv\n",
            "287 conv4_block21_concat\n",
            "288 conv4_block22_0_bn\n",
            "289 conv4_block22_0_relu\n",
            "290 conv4_block22_1_conv\n",
            "291 conv4_block22_1_bn\n",
            "292 conv4_block22_1_relu\n",
            "293 conv4_block22_2_conv\n",
            "294 conv4_block22_concat\n",
            "295 conv4_block23_0_bn\n",
            "296 conv4_block23_0_relu\n",
            "297 conv4_block23_1_conv\n",
            "298 conv4_block23_1_bn\n",
            "299 conv4_block23_1_relu\n",
            "300 conv4_block23_2_conv\n",
            "301 conv4_block23_concat\n",
            "302 conv4_block24_0_bn\n",
            "303 conv4_block24_0_relu\n",
            "304 conv4_block24_1_conv\n",
            "305 conv4_block24_1_bn\n",
            "306 conv4_block24_1_relu\n",
            "307 conv4_block24_2_conv\n",
            "308 conv4_block24_concat\n",
            "309 conv4_block25_0_bn\n",
            "310 conv4_block25_0_relu\n",
            "311 conv4_block25_1_conv\n",
            "312 conv4_block25_1_bn\n",
            "313 conv4_block25_1_relu\n",
            "314 conv4_block25_2_conv\n",
            "315 conv4_block25_concat\n",
            "316 conv4_block26_0_bn\n",
            "317 conv4_block26_0_relu\n",
            "318 conv4_block26_1_conv\n",
            "319 conv4_block26_1_bn\n",
            "320 conv4_block26_1_relu\n",
            "321 conv4_block26_2_conv\n",
            "322 conv4_block26_concat\n",
            "323 conv4_block27_0_bn\n",
            "324 conv4_block27_0_relu\n",
            "325 conv4_block27_1_conv\n",
            "326 conv4_block27_1_bn\n",
            "327 conv4_block27_1_relu\n",
            "328 conv4_block27_2_conv\n",
            "329 conv4_block27_concat\n",
            "330 conv4_block28_0_bn\n",
            "331 conv4_block28_0_relu\n",
            "332 conv4_block28_1_conv\n",
            "333 conv4_block28_1_bn\n",
            "334 conv4_block28_1_relu\n",
            "335 conv4_block28_2_conv\n",
            "336 conv4_block28_concat\n",
            "337 conv4_block29_0_bn\n",
            "338 conv4_block29_0_relu\n",
            "339 conv4_block29_1_conv\n",
            "340 conv4_block29_1_bn\n",
            "341 conv4_block29_1_relu\n",
            "342 conv4_block29_2_conv\n",
            "343 conv4_block29_concat\n",
            "344 conv4_block30_0_bn\n",
            "345 conv4_block30_0_relu\n",
            "346 conv4_block30_1_conv\n",
            "347 conv4_block30_1_bn\n",
            "348 conv4_block30_1_relu\n",
            "349 conv4_block30_2_conv\n",
            "350 conv4_block30_concat\n",
            "351 conv4_block31_0_bn\n",
            "352 conv4_block31_0_relu\n",
            "353 conv4_block31_1_conv\n",
            "354 conv4_block31_1_bn\n",
            "355 conv4_block31_1_relu\n",
            "356 conv4_block31_2_conv\n",
            "357 conv4_block31_concat\n",
            "358 conv4_block32_0_bn\n",
            "359 conv4_block32_0_relu\n",
            "360 conv4_block32_1_conv\n",
            "361 conv4_block32_1_bn\n",
            "362 conv4_block32_1_relu\n",
            "363 conv4_block32_2_conv\n",
            "364 conv4_block32_concat\n",
            "365 conv4_block33_0_bn\n",
            "366 conv4_block33_0_relu\n",
            "367 conv4_block33_1_conv\n",
            "368 conv4_block33_1_bn\n",
            "369 conv4_block33_1_relu\n",
            "370 conv4_block33_2_conv\n",
            "371 conv4_block33_concat\n",
            "372 conv4_block34_0_bn\n",
            "373 conv4_block34_0_relu\n",
            "374 conv4_block34_1_conv\n",
            "375 conv4_block34_1_bn\n",
            "376 conv4_block34_1_relu\n",
            "377 conv4_block34_2_conv\n",
            "378 conv4_block34_concat\n",
            "379 conv4_block35_0_bn\n",
            "380 conv4_block35_0_relu\n",
            "381 conv4_block35_1_conv\n",
            "382 conv4_block35_1_bn\n",
            "383 conv4_block35_1_relu\n",
            "384 conv4_block35_2_conv\n",
            "385 conv4_block35_concat\n",
            "386 conv4_block36_0_bn\n",
            "387 conv4_block36_0_relu\n",
            "388 conv4_block36_1_conv\n",
            "389 conv4_block36_1_bn\n",
            "390 conv4_block36_1_relu\n",
            "391 conv4_block36_2_conv\n",
            "392 conv4_block36_concat\n",
            "393 conv4_block37_0_bn\n",
            "394 conv4_block37_0_relu\n",
            "395 conv4_block37_1_conv\n",
            "396 conv4_block37_1_bn\n",
            "397 conv4_block37_1_relu\n",
            "398 conv4_block37_2_conv\n",
            "399 conv4_block37_concat\n",
            "400 conv4_block38_0_bn\n",
            "401 conv4_block38_0_relu\n",
            "402 conv4_block38_1_conv\n",
            "403 conv4_block38_1_bn\n",
            "404 conv4_block38_1_relu\n",
            "405 conv4_block38_2_conv\n",
            "406 conv4_block38_concat\n",
            "407 conv4_block39_0_bn\n",
            "408 conv4_block39_0_relu\n",
            "409 conv4_block39_1_conv\n",
            "410 conv4_block39_1_bn\n",
            "411 conv4_block39_1_relu\n",
            "412 conv4_block39_2_conv\n",
            "413 conv4_block39_concat\n",
            "414 conv4_block40_0_bn\n",
            "415 conv4_block40_0_relu\n",
            "416 conv4_block40_1_conv\n",
            "417 conv4_block40_1_bn\n",
            "418 conv4_block40_1_relu\n",
            "419 conv4_block40_2_conv\n",
            "420 conv4_block40_concat\n",
            "421 conv4_block41_0_bn\n",
            "422 conv4_block41_0_relu\n",
            "423 conv4_block41_1_conv\n",
            "424 conv4_block41_1_bn\n",
            "425 conv4_block41_1_relu\n",
            "426 conv4_block41_2_conv\n",
            "427 conv4_block41_concat\n",
            "428 conv4_block42_0_bn\n",
            "429 conv4_block42_0_relu\n",
            "430 conv4_block42_1_conv\n",
            "431 conv4_block42_1_bn\n",
            "432 conv4_block42_1_relu\n",
            "433 conv4_block42_2_conv\n",
            "434 conv4_block42_concat\n",
            "435 conv4_block43_0_bn\n",
            "436 conv4_block43_0_relu\n",
            "437 conv4_block43_1_conv\n",
            "438 conv4_block43_1_bn\n",
            "439 conv4_block43_1_relu\n",
            "440 conv4_block43_2_conv\n",
            "441 conv4_block43_concat\n",
            "442 conv4_block44_0_bn\n",
            "443 conv4_block44_0_relu\n",
            "444 conv4_block44_1_conv\n",
            "445 conv4_block44_1_bn\n",
            "446 conv4_block44_1_relu\n",
            "447 conv4_block44_2_conv\n",
            "448 conv4_block44_concat\n",
            "449 conv4_block45_0_bn\n",
            "450 conv4_block45_0_relu\n",
            "451 conv4_block45_1_conv\n",
            "452 conv4_block45_1_bn\n",
            "453 conv4_block45_1_relu\n",
            "454 conv4_block45_2_conv\n",
            "455 conv4_block45_concat\n",
            "456 conv4_block46_0_bn\n",
            "457 conv4_block46_0_relu\n",
            "458 conv4_block46_1_conv\n",
            "459 conv4_block46_1_bn\n",
            "460 conv4_block46_1_relu\n",
            "461 conv4_block46_2_conv\n",
            "462 conv4_block46_concat\n",
            "463 conv4_block47_0_bn\n",
            "464 conv4_block47_0_relu\n",
            "465 conv4_block47_1_conv\n",
            "466 conv4_block47_1_bn\n",
            "467 conv4_block47_1_relu\n",
            "468 conv4_block47_2_conv\n",
            "469 conv4_block47_concat\n",
            "470 conv4_block48_0_bn\n",
            "471 conv4_block48_0_relu\n",
            "472 conv4_block48_1_conv\n",
            "473 conv4_block48_1_bn\n",
            "474 conv4_block48_1_relu\n",
            "475 conv4_block48_2_conv\n",
            "476 conv4_block48_concat\n",
            "477 pool4_bn\n",
            "478 pool4_relu\n",
            "479 pool4_conv\n",
            "480 pool4_pool\n",
            "481 conv5_block1_0_bn\n",
            "482 conv5_block1_0_relu\n",
            "483 conv5_block1_1_conv\n",
            "484 conv5_block1_1_bn\n",
            "485 conv5_block1_1_relu\n",
            "486 conv5_block1_2_conv\n",
            "487 conv5_block1_concat\n",
            "488 conv5_block2_0_bn\n",
            "489 conv5_block2_0_relu\n",
            "490 conv5_block2_1_conv\n",
            "491 conv5_block2_1_bn\n",
            "492 conv5_block2_1_relu\n",
            "493 conv5_block2_2_conv\n",
            "494 conv5_block2_concat\n",
            "495 conv5_block3_0_bn\n",
            "496 conv5_block3_0_relu\n",
            "497 conv5_block3_1_conv\n",
            "498 conv5_block3_1_bn\n",
            "499 conv5_block3_1_relu\n",
            "500 conv5_block3_2_conv\n",
            "501 conv5_block3_concat\n",
            "502 conv5_block4_0_bn\n",
            "503 conv5_block4_0_relu\n",
            "504 conv5_block4_1_conv\n",
            "505 conv5_block4_1_bn\n",
            "506 conv5_block4_1_relu\n",
            "507 conv5_block4_2_conv\n",
            "508 conv5_block4_concat\n",
            "509 conv5_block5_0_bn\n",
            "510 conv5_block5_0_relu\n",
            "511 conv5_block5_1_conv\n",
            "512 conv5_block5_1_bn\n",
            "513 conv5_block5_1_relu\n",
            "514 conv5_block5_2_conv\n",
            "515 conv5_block5_concat\n",
            "516 conv5_block6_0_bn\n",
            "517 conv5_block6_0_relu\n",
            "518 conv5_block6_1_conv\n",
            "519 conv5_block6_1_bn\n",
            "520 conv5_block6_1_relu\n",
            "521 conv5_block6_2_conv\n",
            "522 conv5_block6_concat\n",
            "523 conv5_block7_0_bn\n",
            "524 conv5_block7_0_relu\n",
            "525 conv5_block7_1_conv\n",
            "526 conv5_block7_1_bn\n",
            "527 conv5_block7_1_relu\n",
            "528 conv5_block7_2_conv\n",
            "529 conv5_block7_concat\n",
            "530 conv5_block8_0_bn\n",
            "531 conv5_block8_0_relu\n",
            "532 conv5_block8_1_conv\n",
            "533 conv5_block8_1_bn\n",
            "534 conv5_block8_1_relu\n",
            "535 conv5_block8_2_conv\n",
            "536 conv5_block8_concat\n",
            "537 conv5_block9_0_bn\n",
            "538 conv5_block9_0_relu\n",
            "539 conv5_block9_1_conv\n",
            "540 conv5_block9_1_bn\n",
            "541 conv5_block9_1_relu\n",
            "542 conv5_block9_2_conv\n",
            "543 conv5_block9_concat\n",
            "544 conv5_block10_0_bn\n",
            "545 conv5_block10_0_relu\n",
            "546 conv5_block10_1_conv\n",
            "547 conv5_block10_1_bn\n",
            "548 conv5_block10_1_relu\n",
            "549 conv5_block10_2_conv\n",
            "550 conv5_block10_concat\n",
            "551 conv5_block11_0_bn\n",
            "552 conv5_block11_0_relu\n",
            "553 conv5_block11_1_conv\n",
            "554 conv5_block11_1_bn\n",
            "555 conv5_block11_1_relu\n",
            "556 conv5_block11_2_conv\n",
            "557 conv5_block11_concat\n",
            "558 conv5_block12_0_bn\n",
            "559 conv5_block12_0_relu\n",
            "560 conv5_block12_1_conv\n",
            "561 conv5_block12_1_bn\n",
            "562 conv5_block12_1_relu\n",
            "563 conv5_block12_2_conv\n",
            "564 conv5_block12_concat\n",
            "565 conv5_block13_0_bn\n",
            "566 conv5_block13_0_relu\n",
            "567 conv5_block13_1_conv\n",
            "568 conv5_block13_1_bn\n",
            "569 conv5_block13_1_relu\n",
            "570 conv5_block13_2_conv\n",
            "571 conv5_block13_concat\n",
            "572 conv5_block14_0_bn\n",
            "573 conv5_block14_0_relu\n",
            "574 conv5_block14_1_conv\n",
            "575 conv5_block14_1_bn\n",
            "576 conv5_block14_1_relu\n",
            "577 conv5_block14_2_conv\n",
            "578 conv5_block14_concat\n",
            "579 conv5_block15_0_bn\n",
            "580 conv5_block15_0_relu\n",
            "581 conv5_block15_1_conv\n",
            "582 conv5_block15_1_bn\n",
            "583 conv5_block15_1_relu\n",
            "584 conv5_block15_2_conv\n",
            "585 conv5_block15_concat\n",
            "586 conv5_block16_0_bn\n",
            "587 conv5_block16_0_relu\n",
            "588 conv5_block16_1_conv\n",
            "589 conv5_block16_1_bn\n",
            "590 conv5_block16_1_relu\n",
            "591 conv5_block16_2_conv\n",
            "592 conv5_block16_concat\n",
            "593 conv5_block17_0_bn\n",
            "594 conv5_block17_0_relu\n",
            "595 conv5_block17_1_conv\n",
            "596 conv5_block17_1_bn\n",
            "597 conv5_block17_1_relu\n",
            "598 conv5_block17_2_conv\n",
            "599 conv5_block17_concat\n",
            "600 conv5_block18_0_bn\n",
            "601 conv5_block18_0_relu\n",
            "602 conv5_block18_1_conv\n",
            "603 conv5_block18_1_bn\n",
            "604 conv5_block18_1_relu\n",
            "605 conv5_block18_2_conv\n",
            "606 conv5_block18_concat\n",
            "607 conv5_block19_0_bn\n",
            "608 conv5_block19_0_relu\n",
            "609 conv5_block19_1_conv\n",
            "610 conv5_block19_1_bn\n",
            "611 conv5_block19_1_relu\n",
            "612 conv5_block19_2_conv\n",
            "613 conv5_block19_concat\n",
            "614 conv5_block20_0_bn\n",
            "615 conv5_block20_0_relu\n",
            "616 conv5_block20_1_conv\n",
            "617 conv5_block20_1_bn\n",
            "618 conv5_block20_1_relu\n",
            "619 conv5_block20_2_conv\n",
            "620 conv5_block20_concat\n",
            "621 conv5_block21_0_bn\n",
            "622 conv5_block21_0_relu\n",
            "623 conv5_block21_1_conv\n",
            "624 conv5_block21_1_bn\n",
            "625 conv5_block21_1_relu\n",
            "626 conv5_block21_2_conv\n",
            "627 conv5_block21_concat\n",
            "628 conv5_block22_0_bn\n",
            "629 conv5_block22_0_relu\n",
            "630 conv5_block22_1_conv\n",
            "631 conv5_block22_1_bn\n",
            "632 conv5_block22_1_relu\n",
            "633 conv5_block22_2_conv\n",
            "634 conv5_block22_concat\n",
            "635 conv5_block23_0_bn\n",
            "636 conv5_block23_0_relu\n",
            "637 conv5_block23_1_conv\n",
            "638 conv5_block23_1_bn\n",
            "639 conv5_block23_1_relu\n",
            "640 conv5_block23_2_conv\n",
            "641 conv5_block23_concat\n",
            "642 conv5_block24_0_bn\n",
            "643 conv5_block24_0_relu\n",
            "644 conv5_block24_1_conv\n",
            "645 conv5_block24_1_bn\n",
            "646 conv5_block24_1_relu\n",
            "647 conv5_block24_2_conv\n",
            "648 conv5_block24_concat\n",
            "649 conv5_block25_0_bn\n",
            "650 conv5_block25_0_relu\n",
            "651 conv5_block25_1_conv\n",
            "652 conv5_block25_1_bn\n",
            "653 conv5_block25_1_relu\n",
            "654 conv5_block25_2_conv\n",
            "655 conv5_block25_concat\n",
            "656 conv5_block26_0_bn\n",
            "657 conv5_block26_0_relu\n",
            "658 conv5_block26_1_conv\n",
            "659 conv5_block26_1_bn\n",
            "660 conv5_block26_1_relu\n",
            "661 conv5_block26_2_conv\n",
            "662 conv5_block26_concat\n",
            "663 conv5_block27_0_bn\n",
            "664 conv5_block27_0_relu\n",
            "665 conv5_block27_1_conv\n",
            "666 conv5_block27_1_bn\n",
            "667 conv5_block27_1_relu\n",
            "668 conv5_block27_2_conv\n",
            "669 conv5_block27_concat\n",
            "670 conv5_block28_0_bn\n",
            "671 conv5_block28_0_relu\n",
            "672 conv5_block28_1_conv\n",
            "673 conv5_block28_1_bn\n",
            "674 conv5_block28_1_relu\n",
            "675 conv5_block28_2_conv\n",
            "676 conv5_block28_concat\n",
            "677 conv5_block29_0_bn\n",
            "678 conv5_block29_0_relu\n",
            "679 conv5_block29_1_conv\n",
            "680 conv5_block29_1_bn\n",
            "681 conv5_block29_1_relu\n",
            "682 conv5_block29_2_conv\n",
            "683 conv5_block29_concat\n",
            "684 conv5_block30_0_bn\n",
            "685 conv5_block30_0_relu\n",
            "686 conv5_block30_1_conv\n",
            "687 conv5_block30_1_bn\n",
            "688 conv5_block30_1_relu\n",
            "689 conv5_block30_2_conv\n",
            "690 conv5_block30_concat\n",
            "691 conv5_block31_0_bn\n",
            "692 conv5_block31_0_relu\n",
            "693 conv5_block31_1_conv\n",
            "694 conv5_block31_1_bn\n",
            "695 conv5_block31_1_relu\n",
            "696 conv5_block31_2_conv\n",
            "697 conv5_block31_concat\n",
            "698 conv5_block32_0_bn\n",
            "699 conv5_block32_0_relu\n",
            "700 conv5_block32_1_conv\n",
            "701 conv5_block32_1_bn\n",
            "702 conv5_block32_1_relu\n",
            "703 conv5_block32_2_conv\n",
            "704 conv5_block32_concat\n",
            "705 bn\n",
            "706 relu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y3xP9vvZb2q9",
        "colab_type": "code",
        "outputId": "30f15b62-7cbc-483f-9631-2db5bfdfdf67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5090
        }
      },
      "cell_type": "code",
      "source": [
        "# we will freeze the first 7 layers and unfreeze the rest:\n",
        "for layer in model.layers[:0]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[0:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "# we need to recompile the model for these modifications to take effect\n",
        "# we use SGD with a low learning rate\n",
        "from keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(lr=0.00001, momentum=0.9), loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
        "# alongside the top Dense layers\n",
        "model.fit(densenet_input_train, to_categorical(y_train, num_classes=2), batch_size=64, epochs=150,\n",
        "          validation_data=(densenet_input_test, to_categorical(y_test, num_classes=2)),\n",
        "          verbose=2, shuffle=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5000 samples, validate on 1000 samples\n",
            "Epoch 1/150\n",
            " - 47s - loss: 0.4420 - acc: 0.7924 - val_loss: 0.5372 - val_acc: 0.7640\n",
            "Epoch 2/150\n",
            " - 14s - loss: 0.4195 - acc: 0.8104 - val_loss: 0.5125 - val_acc: 0.7750\n",
            "Epoch 3/150\n",
            " - 14s - loss: 0.3878 - acc: 0.8312 - val_loss: 0.5049 - val_acc: 0.7830\n",
            "Epoch 4/150\n",
            " - 15s - loss: 0.3719 - acc: 0.8362 - val_loss: 0.4830 - val_acc: 0.7870\n",
            "Epoch 5/150\n",
            " - 14s - loss: 0.3587 - acc: 0.8424 - val_loss: 0.4664 - val_acc: 0.7860\n",
            "Epoch 6/150\n",
            " - 14s - loss: 0.3350 - acc: 0.8592 - val_loss: 0.4539 - val_acc: 0.7850\n",
            "Epoch 7/150\n",
            " - 14s - loss: 0.3349 - acc: 0.8592 - val_loss: 0.4402 - val_acc: 0.7920\n",
            "Epoch 8/150\n",
            " - 14s - loss: 0.3242 - acc: 0.8614 - val_loss: 0.4341 - val_acc: 0.8060\n",
            "Epoch 9/150\n",
            " - 15s - loss: 0.3095 - acc: 0.8718 - val_loss: 0.4333 - val_acc: 0.8030\n",
            "Epoch 10/150\n",
            " - 14s - loss: 0.3148 - acc: 0.8664 - val_loss: 0.4205 - val_acc: 0.8050\n",
            "Epoch 11/150\n",
            " - 14s - loss: 0.2927 - acc: 0.8788 - val_loss: 0.4121 - val_acc: 0.8170\n",
            "Epoch 12/150\n",
            " - 14s - loss: 0.2863 - acc: 0.8818 - val_loss: 0.4095 - val_acc: 0.8170\n",
            "Epoch 13/150\n",
            " - 14s - loss: 0.2775 - acc: 0.8828 - val_loss: 0.4159 - val_acc: 0.8150\n",
            "Epoch 14/150\n",
            " - 14s - loss: 0.2778 - acc: 0.8806 - val_loss: 0.4076 - val_acc: 0.8200\n",
            "Epoch 15/150\n",
            " - 15s - loss: 0.2702 - acc: 0.8850 - val_loss: 0.4022 - val_acc: 0.8200\n",
            "Epoch 16/150\n",
            " - 14s - loss: 0.2597 - acc: 0.8980 - val_loss: 0.3955 - val_acc: 0.8290\n",
            "Epoch 17/150\n",
            " - 14s - loss: 0.2633 - acc: 0.8932 - val_loss: 0.3957 - val_acc: 0.8250\n",
            "Epoch 18/150\n",
            " - 14s - loss: 0.2563 - acc: 0.8990 - val_loss: 0.3945 - val_acc: 0.8280\n",
            "Epoch 19/150\n",
            " - 14s - loss: 0.2590 - acc: 0.8960 - val_loss: 0.3931 - val_acc: 0.8320\n",
            "Epoch 20/150\n",
            " - 15s - loss: 0.2481 - acc: 0.9008 - val_loss: 0.3873 - val_acc: 0.8370\n",
            "Epoch 21/150\n",
            " - 15s - loss: 0.2447 - acc: 0.9040 - val_loss: 0.3881 - val_acc: 0.8430\n",
            "Epoch 22/150\n",
            " - 15s - loss: 0.2397 - acc: 0.9036 - val_loss: 0.3808 - val_acc: 0.8420\n",
            "Epoch 23/150\n",
            " - 14s - loss: 0.2329 - acc: 0.9100 - val_loss: 0.3791 - val_acc: 0.8430\n",
            "Epoch 24/150\n",
            " - 14s - loss: 0.2249 - acc: 0.9126 - val_loss: 0.3778 - val_acc: 0.8440\n",
            "Epoch 25/150\n",
            " - 14s - loss: 0.2286 - acc: 0.9100 - val_loss: 0.3759 - val_acc: 0.8440\n",
            "Epoch 26/150\n",
            " - 15s - loss: 0.2212 - acc: 0.9172 - val_loss: 0.3710 - val_acc: 0.8440\n",
            "Epoch 27/150\n",
            " - 14s - loss: 0.2139 - acc: 0.9198 - val_loss: 0.3732 - val_acc: 0.8400\n",
            "Epoch 28/150\n",
            " - 14s - loss: 0.2164 - acc: 0.9138 - val_loss: 0.3692 - val_acc: 0.8450\n",
            "Epoch 29/150\n",
            " - 14s - loss: 0.2166 - acc: 0.9174 - val_loss: 0.3693 - val_acc: 0.8440\n",
            "Epoch 30/150\n",
            " - 14s - loss: 0.2084 - acc: 0.9194 - val_loss: 0.3668 - val_acc: 0.8450\n",
            "Epoch 31/150\n",
            " - 15s - loss: 0.2043 - acc: 0.9220 - val_loss: 0.3672 - val_acc: 0.8500\n",
            "Epoch 32/150\n",
            " - 15s - loss: 0.2044 - acc: 0.9252 - val_loss: 0.3620 - val_acc: 0.8540\n",
            "Epoch 33/150\n",
            " - 14s - loss: 0.1941 - acc: 0.9260 - val_loss: 0.3586 - val_acc: 0.8550\n",
            "Epoch 34/150\n",
            " - 14s - loss: 0.1931 - acc: 0.9246 - val_loss: 0.3593 - val_acc: 0.8560\n",
            "Epoch 35/150\n",
            " - 14s - loss: 0.1970 - acc: 0.9280 - val_loss: 0.3578 - val_acc: 0.8550\n",
            "Epoch 36/150\n",
            " - 14s - loss: 0.1906 - acc: 0.9326 - val_loss: 0.3570 - val_acc: 0.8590\n",
            "Epoch 37/150\n",
            " - 15s - loss: 0.1875 - acc: 0.9322 - val_loss: 0.3520 - val_acc: 0.8610\n",
            "Epoch 38/150\n",
            " - 14s - loss: 0.1889 - acc: 0.9290 - val_loss: 0.3487 - val_acc: 0.8620\n",
            "Epoch 39/150\n",
            " - 15s - loss: 0.1802 - acc: 0.9338 - val_loss: 0.3511 - val_acc: 0.8600\n",
            "Epoch 40/150\n",
            " - 14s - loss: 0.1797 - acc: 0.9348 - val_loss: 0.3478 - val_acc: 0.8650\n",
            "Epoch 41/150\n",
            " - 14s - loss: 0.1717 - acc: 0.9364 - val_loss: 0.3479 - val_acc: 0.8590\n",
            "Epoch 42/150\n",
            " - 14s - loss: 0.1739 - acc: 0.9370 - val_loss: 0.3465 - val_acc: 0.8610\n",
            "Epoch 43/150\n",
            " - 15s - loss: 0.1729 - acc: 0.9366 - val_loss: 0.3462 - val_acc: 0.8640\n",
            "Epoch 44/150\n",
            " - 15s - loss: 0.1715 - acc: 0.9376 - val_loss: 0.3480 - val_acc: 0.8600\n",
            "Epoch 45/150\n",
            " - 14s - loss: 0.1710 - acc: 0.9394 - val_loss: 0.3510 - val_acc: 0.8580\n",
            "Epoch 46/150\n",
            " - 14s - loss: 0.1702 - acc: 0.9406 - val_loss: 0.3440 - val_acc: 0.8630\n",
            "Epoch 47/150\n",
            " - 14s - loss: 0.1646 - acc: 0.9398 - val_loss: 0.3376 - val_acc: 0.8650\n",
            "Epoch 48/150\n",
            " - 15s - loss: 0.1525 - acc: 0.9476 - val_loss: 0.3389 - val_acc: 0.8650\n",
            "Epoch 49/150\n",
            " - 14s - loss: 0.1560 - acc: 0.9424 - val_loss: 0.3379 - val_acc: 0.8620\n",
            "Epoch 50/150\n",
            " - 14s - loss: 0.1504 - acc: 0.9520 - val_loss: 0.3385 - val_acc: 0.8650\n",
            "Epoch 51/150\n",
            " - 14s - loss: 0.1525 - acc: 0.9466 - val_loss: 0.3386 - val_acc: 0.8630\n",
            "Epoch 52/150\n",
            " - 14s - loss: 0.1501 - acc: 0.9504 - val_loss: 0.3378 - val_acc: 0.8690\n",
            "Epoch 53/150\n",
            " - 14s - loss: 0.1433 - acc: 0.9516 - val_loss: 0.3370 - val_acc: 0.8670\n",
            "Epoch 54/150\n",
            " - 15s - loss: 0.1437 - acc: 0.9500 - val_loss: 0.3337 - val_acc: 0.8680\n",
            "Epoch 55/150\n",
            " - 14s - loss: 0.1399 - acc: 0.9562 - val_loss: 0.3352 - val_acc: 0.8700\n",
            "Epoch 56/150\n",
            " - 14s - loss: 0.1367 - acc: 0.9536 - val_loss: 0.3349 - val_acc: 0.8640\n",
            "Epoch 57/150\n",
            " - 14s - loss: 0.1405 - acc: 0.9530 - val_loss: 0.3331 - val_acc: 0.8710\n",
            "Epoch 58/150\n",
            " - 14s - loss: 0.1379 - acc: 0.9538 - val_loss: 0.3314 - val_acc: 0.8710\n",
            "Epoch 59/150\n",
            " - 15s - loss: 0.1285 - acc: 0.9594 - val_loss: 0.3295 - val_acc: 0.8720\n",
            "Epoch 60/150\n",
            " - 15s - loss: 0.1338 - acc: 0.9554 - val_loss: 0.3313 - val_acc: 0.8710\n",
            "Epoch 61/150\n",
            " - 14s - loss: 0.1289 - acc: 0.9556 - val_loss: 0.3308 - val_acc: 0.8710\n",
            "Epoch 62/150\n",
            " - 14s - loss: 0.1328 - acc: 0.9548 - val_loss: 0.3316 - val_acc: 0.8700\n",
            "Epoch 63/150\n",
            " - 14s - loss: 0.1256 - acc: 0.9606 - val_loss: 0.3312 - val_acc: 0.8700\n",
            "Epoch 64/150\n",
            " - 14s - loss: 0.1235 - acc: 0.9604 - val_loss: 0.3297 - val_acc: 0.8730\n",
            "Epoch 65/150\n",
            " - 16s - loss: 0.1232 - acc: 0.9620 - val_loss: 0.3168 - val_acc: 0.8750\n",
            "Epoch 66/150\n",
            " - 14s - loss: 0.1225 - acc: 0.9580 - val_loss: 0.3191 - val_acc: 0.8740\n",
            "Epoch 67/150\n",
            " - 14s - loss: 0.1219 - acc: 0.9630 - val_loss: 0.3200 - val_acc: 0.8760\n",
            "Epoch 68/150\n",
            " - 14s - loss: 0.1205 - acc: 0.9624 - val_loss: 0.3202 - val_acc: 0.8740\n",
            "Epoch 69/150\n",
            " - 14s - loss: 0.1255 - acc: 0.9572 - val_loss: 0.3200 - val_acc: 0.8760\n",
            "Epoch 70/150\n",
            " - 14s - loss: 0.1140 - acc: 0.9640 - val_loss: 0.3202 - val_acc: 0.8750\n",
            "Epoch 71/150\n",
            " - 15s - loss: 0.1134 - acc: 0.9650 - val_loss: 0.3188 - val_acc: 0.8760\n",
            "Epoch 72/150\n",
            " - 14s - loss: 0.1143 - acc: 0.9652 - val_loss: 0.3177 - val_acc: 0.8780\n",
            "Epoch 73/150\n",
            " - 14s - loss: 0.1110 - acc: 0.9642 - val_loss: 0.3209 - val_acc: 0.8790\n",
            "Epoch 74/150\n",
            " - 14s - loss: 0.1138 - acc: 0.9602 - val_loss: 0.3183 - val_acc: 0.8820\n",
            "Epoch 75/150\n",
            " - 14s - loss: 0.1035 - acc: 0.9702 - val_loss: 0.3151 - val_acc: 0.8800\n",
            "Epoch 76/150\n",
            " - 15s - loss: 0.1053 - acc: 0.9670 - val_loss: 0.3146 - val_acc: 0.8810\n",
            "Epoch 77/150\n",
            " - 14s - loss: 0.1056 - acc: 0.9658 - val_loss: 0.3115 - val_acc: 0.8800\n",
            "Epoch 78/150\n",
            " - 14s - loss: 0.1004 - acc: 0.9684 - val_loss: 0.3100 - val_acc: 0.8830\n",
            "Epoch 79/150\n",
            " - 14s - loss: 0.1059 - acc: 0.9678 - val_loss: 0.3121 - val_acc: 0.8840\n",
            "Epoch 80/150\n",
            " - 14s - loss: 0.1089 - acc: 0.9628 - val_loss: 0.3102 - val_acc: 0.8790\n",
            "Epoch 81/150\n",
            " - 15s - loss: 0.0938 - acc: 0.9736 - val_loss: 0.3091 - val_acc: 0.8810\n",
            "Epoch 82/150\n",
            " - 15s - loss: 0.1020 - acc: 0.9718 - val_loss: 0.3055 - val_acc: 0.8860\n",
            "Epoch 83/150\n",
            " - 14s - loss: 0.1022 - acc: 0.9668 - val_loss: 0.3071 - val_acc: 0.8860\n",
            "Epoch 84/150\n",
            " - 14s - loss: 0.0922 - acc: 0.9744 - val_loss: 0.3081 - val_acc: 0.8890\n",
            "Epoch 85/150\n",
            " - 14s - loss: 0.0991 - acc: 0.9678 - val_loss: 0.3086 - val_acc: 0.8830\n",
            "Epoch 86/150\n",
            " - 14s - loss: 0.0916 - acc: 0.9732 - val_loss: 0.3103 - val_acc: 0.8870\n",
            "Epoch 87/150\n",
            " - 16s - loss: 0.0926 - acc: 0.9746 - val_loss: 0.3114 - val_acc: 0.8860\n",
            "Epoch 88/150\n",
            " - 14s - loss: 0.0929 - acc: 0.9706 - val_loss: 0.3132 - val_acc: 0.8890\n",
            "Epoch 89/150\n",
            " - 14s - loss: 0.0907 - acc: 0.9756 - val_loss: 0.3140 - val_acc: 0.8850\n",
            "Epoch 90/150\n",
            " - 14s - loss: 0.0872 - acc: 0.9742 - val_loss: 0.3137 - val_acc: 0.8900\n",
            "Epoch 91/150\n",
            " - 14s - loss: 0.0910 - acc: 0.9724 - val_loss: 0.3145 - val_acc: 0.8910\n",
            "Epoch 92/150\n",
            " - 14s - loss: 0.0954 - acc: 0.9738 - val_loss: 0.3144 - val_acc: 0.8890\n",
            "Epoch 93/150\n",
            " - 15s - loss: 0.0866 - acc: 0.9748 - val_loss: 0.3139 - val_acc: 0.8890\n",
            "Epoch 94/150\n",
            " - 14s - loss: 0.0887 - acc: 0.9746 - val_loss: 0.3135 - val_acc: 0.8930\n",
            "Epoch 95/150\n",
            " - 14s - loss: 0.0849 - acc: 0.9762 - val_loss: 0.3119 - val_acc: 0.8920\n",
            "Epoch 96/150\n",
            " - 14s - loss: 0.0859 - acc: 0.9762 - val_loss: 0.3142 - val_acc: 0.8890\n",
            "Epoch 97/150\n",
            " - 14s - loss: 0.0861 - acc: 0.9746 - val_loss: 0.3143 - val_acc: 0.8880\n",
            "Epoch 98/150\n",
            " - 15s - loss: 0.0842 - acc: 0.9748 - val_loss: 0.3137 - val_acc: 0.8870\n",
            "Epoch 99/150\n",
            " - 15s - loss: 0.0859 - acc: 0.9748 - val_loss: 0.3138 - val_acc: 0.8900\n",
            "Epoch 100/150\n",
            " - 14s - loss: 0.0804 - acc: 0.9770 - val_loss: 0.3140 - val_acc: 0.8900\n",
            "Epoch 101/150\n",
            " - 14s - loss: 0.0817 - acc: 0.9780 - val_loss: 0.3178 - val_acc: 0.8890\n",
            "Epoch 102/150\n",
            " - 15s - loss: 0.0776 - acc: 0.9776 - val_loss: 0.3163 - val_acc: 0.8890\n",
            "Epoch 103/150\n",
            " - 14s - loss: 0.0774 - acc: 0.9784 - val_loss: 0.3170 - val_acc: 0.8870\n",
            "Epoch 104/150\n",
            " - 15s - loss: 0.0809 - acc: 0.9758 - val_loss: 0.3127 - val_acc: 0.8860\n",
            "Epoch 105/150\n",
            " - 14s - loss: 0.0790 - acc: 0.9770 - val_loss: 0.3099 - val_acc: 0.8890\n",
            "Epoch 106/150\n",
            " - 14s - loss: 0.0746 - acc: 0.9806 - val_loss: 0.3088 - val_acc: 0.8900\n",
            "Epoch 107/150\n",
            " - 14s - loss: 0.0725 - acc: 0.9806 - val_loss: 0.3106 - val_acc: 0.8900\n",
            "Epoch 108/150\n",
            " - 16s - loss: 0.0744 - acc: 0.9802 - val_loss: 0.3098 - val_acc: 0.8890\n",
            "Epoch 109/150\n",
            " - 15s - loss: 0.0740 - acc: 0.9802 - val_loss: 0.3100 - val_acc: 0.8920\n",
            "Epoch 110/150\n",
            " - 15s - loss: 0.0688 - acc: 0.9838 - val_loss: 0.3099 - val_acc: 0.8950\n",
            "Epoch 111/150\n",
            " - 14s - loss: 0.0699 - acc: 0.9824 - val_loss: 0.3071 - val_acc: 0.8960\n",
            "Epoch 112/150\n",
            " - 14s - loss: 0.0676 - acc: 0.9818 - val_loss: 0.3071 - val_acc: 0.8970\n",
            "Epoch 113/150\n",
            " - 14s - loss: 0.0692 - acc: 0.9792 - val_loss: 0.3077 - val_acc: 0.8970\n",
            "Epoch 114/150\n",
            " - 14s - loss: 0.0693 - acc: 0.9814 - val_loss: 0.3062 - val_acc: 0.8960\n",
            "Epoch 115/150\n",
            " - 15s - loss: 0.0648 - acc: 0.9850 - val_loss: 0.3055 - val_acc: 0.8910\n",
            "Epoch 116/150\n",
            " - 14s - loss: 0.0696 - acc: 0.9804 - val_loss: 0.3072 - val_acc: 0.8930\n",
            "Epoch 117/150\n",
            " - 14s - loss: 0.0688 - acc: 0.9814 - val_loss: 0.3044 - val_acc: 0.8930\n",
            "Epoch 118/150\n",
            " - 14s - loss: 0.0618 - acc: 0.9876 - val_loss: 0.3052 - val_acc: 0.8930\n",
            "Epoch 119/150\n",
            " - 14s - loss: 0.0669 - acc: 0.9802 - val_loss: 0.3045 - val_acc: 0.8940\n",
            "Epoch 120/150\n",
            " - 14s - loss: 0.0681 - acc: 0.9818 - val_loss: 0.3027 - val_acc: 0.8920\n",
            "Epoch 121/150\n",
            " - 15s - loss: 0.0577 - acc: 0.9870 - val_loss: 0.3021 - val_acc: 0.8950\n",
            "Epoch 122/150\n",
            " - 14s - loss: 0.0638 - acc: 0.9820 - val_loss: 0.3037 - val_acc: 0.8960\n",
            "Epoch 123/150\n",
            " - 15s - loss: 0.0614 - acc: 0.9844 - val_loss: 0.3019 - val_acc: 0.8920\n",
            "Epoch 124/150\n",
            " - 14s - loss: 0.0628 - acc: 0.9836 - val_loss: 0.2980 - val_acc: 0.8940\n",
            "Epoch 125/150\n",
            " - 14s - loss: 0.0639 - acc: 0.9844 - val_loss: 0.2967 - val_acc: 0.8970\n",
            "Epoch 126/150\n",
            " - 15s - loss: 0.0718 - acc: 0.9774 - val_loss: 0.2968 - val_acc: 0.8950\n",
            "Epoch 127/150\n",
            " - 14s - loss: 0.0605 - acc: 0.9858 - val_loss: 0.2989 - val_acc: 0.8960\n",
            "Epoch 128/150\n",
            " - 14s - loss: 0.0590 - acc: 0.9856 - val_loss: 0.2983 - val_acc: 0.8960\n",
            "Epoch 129/150\n",
            " - 15s - loss: 0.0633 - acc: 0.9850 - val_loss: 0.2981 - val_acc: 0.8960\n",
            "Epoch 130/150\n",
            " - 15s - loss: 0.0595 - acc: 0.9856 - val_loss: 0.2994 - val_acc: 0.8980\n",
            "Epoch 131/150\n",
            " - 14s - loss: 0.0581 - acc: 0.9852 - val_loss: 0.2995 - val_acc: 0.8960\n",
            "Epoch 132/150\n",
            " - 15s - loss: 0.0653 - acc: 0.9794 - val_loss: 0.3043 - val_acc: 0.8900\n",
            "Epoch 133/150\n",
            " - 14s - loss: 0.0606 - acc: 0.9842 - val_loss: 0.3045 - val_acc: 0.8880\n",
            "Epoch 134/150\n",
            " - 14s - loss: 0.0671 - acc: 0.9776 - val_loss: 0.3012 - val_acc: 0.8970\n",
            "Epoch 135/150\n",
            " - 14s - loss: 0.0635 - acc: 0.9826 - val_loss: 0.3010 - val_acc: 0.8960\n",
            "Epoch 136/150\n",
            " - 14s - loss: 0.0619 - acc: 0.9816 - val_loss: 0.3019 - val_acc: 0.8950\n",
            "Epoch 137/150\n",
            " - 15s - loss: 0.0609 - acc: 0.9822 - val_loss: 0.3010 - val_acc: 0.8960\n",
            "Epoch 138/150\n",
            " - 14s - loss: 0.0593 - acc: 0.9842 - val_loss: 0.2996 - val_acc: 0.8950\n",
            "Epoch 139/150\n",
            " - 14s - loss: 0.0562 - acc: 0.9852 - val_loss: 0.3003 - val_acc: 0.8950\n",
            "Epoch 140/150\n",
            " - 14s - loss: 0.0580 - acc: 0.9852 - val_loss: 0.3015 - val_acc: 0.8920\n",
            "Epoch 141/150\n",
            " - 14s - loss: 0.0612 - acc: 0.9810 - val_loss: 0.3019 - val_acc: 0.8960\n",
            "Epoch 142/150\n",
            " - 14s - loss: 0.0544 - acc: 0.9862 - val_loss: 0.3018 - val_acc: 0.8950\n",
            "Epoch 143/150\n",
            " - 15s - loss: 0.0546 - acc: 0.9858 - val_loss: 0.3024 - val_acc: 0.8940\n",
            "Epoch 144/150\n",
            " - 15s - loss: 0.0523 - acc: 0.9884 - val_loss: 0.3043 - val_acc: 0.8930\n",
            "Epoch 145/150\n",
            " - 14s - loss: 0.0520 - acc: 0.9880 - val_loss: 0.3021 - val_acc: 0.8940\n",
            "Epoch 146/150\n",
            " - 14s - loss: 0.0547 - acc: 0.9868 - val_loss: 0.3008 - val_acc: 0.8990\n",
            "Epoch 147/150\n",
            " - 14s - loss: 0.0482 - acc: 0.9894 - val_loss: 0.2994 - val_acc: 0.8980\n",
            "Epoch 148/150\n",
            " - 15s - loss: 0.0467 - acc: 0.9906 - val_loss: 0.3006 - val_acc: 0.8990\n",
            "Epoch 149/150\n",
            " - 15s - loss: 0.0524 - acc: 0.9872 - val_loss: 0.3015 - val_acc: 0.9000\n",
            "Epoch 150/150\n",
            " - 14s - loss: 0.0523 - acc: 0.9866 - val_loss: 0.3017 - val_acc: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f55631b3ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}